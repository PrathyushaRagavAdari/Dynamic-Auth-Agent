{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ou7ldunfgibv3dctxoqa",
   "authorId": "6426757998193",
   "authorName": "FLAMINGO",
   "authorEmail": "",
   "sessionId": "dff3c0a3-a0ee-4864-816c-9040b12496ae",
   "lastEditTime": 1771826983383
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "-- Welcome to Snowflake Notebooks!\n-- Try out a SQL cell to generate some data.\nSELECT 'FRIDAY' as SNOWDAY, 0.2 as CHANCE_OF_SNOW\nUNION ALL\nSELECT 'SATURDAY',0.5\nUNION ALL \nSELECT 'SUNDAY', 0.9;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Then, we can use the python name to turn cell2 into a Pandas dataframe\nmy_df = cell2.to_pandas()\n\n# Chart the data\nst.subheader(\"Chance of SNOW â„ï¸\")\nst.line_chart(my_df, x='SNOWDAY', y='CHANCE_OF_SNOW')\n\n# Give it a go!\nst.subheader(\"Try it out yourself and show off your skills ðŸ¥‡\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f5e8e699-0ee7-4b02-8f16-bd14b623a6c3",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- 1. Create the database and schema for GraphGuard\nCREATE DATABASE IF NOT EXISTS GRAPHGUARD_DB;\nUSE DATABASE GRAPHGUARD_DB;\n\nCREATE SCHEMA IF NOT EXISTS TRANSACTIONS;\nUSE SCHEMA TRANSACTIONS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30c252aa-8042-4620-bb2f-86ae14c5c2f7",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- 2. Create the Raw Transactions Table\nCREATE TABLE IF NOT EXISTS RAW_TRANSACTIONS (\n    TRANSACTION_ID VARCHAR,\n    USER_ID VARCHAR,\n    CARD_ID VARCHAR,\n    TXN_YEAR INT,\n    TXN_MONTH INT,\n    TXN_DAY INT,\n    TXN_TIME VARCHAR,\n    AMOUNT FLOAT,\n    MERCHANT_NAME VARCHAR,\n    MERCHANT_CITY VARCHAR,\n    MERCHANT_STATE VARCHAR,\n    MERCHANT_ZIP VARCHAR,\n    MCC_CODE VARCHAR,\n    IS_FRAUD BOOLEAN\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6335a553-ebf4-4ca2-b79d-c2ab1cb3f9bf",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "-- 3. Create a view that automatically structures recent valid transactions for our LLM\nCREATE OR REPLACE VIEW RECENT_USER_ACTIVITY AS\nSELECT \n    USER_ID,\n    MERCHANT_NAME,\n    MERCHANT_CITY,\n    AMOUNT,\n    TXN_YEAR || '-' || TXN_MONTH || '-' || TXN_DAY AS TXN_DATE\nFROM RAW_TRANSACTIONS\nWHERE IS_FRAUD = FALSE\nORDER BY TXN_YEAR DESC, TXN_MONTH DESC, TXN_DAY DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9bba86d-6609-4b27-baf4-92bae135db59",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "-- 4. Insert dummy data to simulate the Kaggle dataset\nINSERT INTO RAW_TRANSACTIONS (TRANSACTION_ID, USER_ID, TXN_YEAR, TXN_MONTH, TXN_DAY, AMOUNT, MERCHANT_NAME, MERCHANT_CITY, IS_FRAUD)\nVALUES \n('TXN1', 'USER_9981', 2026, 2, 21, 54.30, 'Kroger', 'Columbus, OH', FALSE),\n('TXN2', 'USER_9981', 2026, 2, 20, 12.50, 'Uber', 'Columbus, OH', FALSE),\n('TXN3', 'USER_4412', 2026, 2, 22, 8.99, 'Starbucks', 'Seattle, WA', FALSE);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea662e16-9754-41a6-a9bc-260178e41db5",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "import snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import col\nimport json\n\n# Snowflake Notebooks automatically provide a 'session' object\nsession = snowpark.Session.builder.getOrCreate()\n\ndef get_user_transaction_graph(user_id: str) -> str:\n    \"\"\"Fetches recent transactions from the Snowflake view to build GraphRAG context.\"\"\"\n    \n    # Query the Feature View using Snowpark\n    df = session.table(\"GRAPHGUARD_DB.TRANSACTIONS.RECENT_USER_ACTIVITY\") \\\n                .filter(col(\"USER_ID\") == user_id) \\\n                .limit(5)\n    \n    # Convert to Pandas for easy JSON serialization\n    pdf = df.to_pandas()\n    \n    if pdf.empty:\n        return json.dumps({\"user_id\": user_id, \"error\": \"No recent valid transactions found.\"})\n    \n    # Build the 'Nodes' for the GraphRAG\n    recent_nodes = []\n    for _, row in pdf.iterrows():\n        recent_nodes.append({\n            \"merchant\": row[\"MERCHANT_NAME\"],\n            \"location\": row[\"MERCHANT_CITY\"],\n            \"amount\": float(row[\"AMOUNT\"]),\n            \"date\": row[\"TXN_DATE\"]\n        })\n        \n    graph_context = {\n        \"user_id\": user_id,\n        \"recent_nodes\": recent_nodes\n    }\n    \n    return json.dumps(graph_context, indent=2)\n\n# --- Test the Pipeline ---\nprint(\"--- GRAPHGUARD FEATURE EXTRACTION TEST ---\")\ntest_context = get_user_transaction_graph(\"USER_9981\")\nprint(f\"\\nExtracted JSON Context for LLM prompt:\\n{test_context}\")",
   "execution_count": null
  }
 ]
}